# flink内核分析

## 深入分析flink的网络栈

参考链接：https://flink.apache.org/2019/06/05/flink-network-stack.html

flink的网络栈是flink-runtime模块的核心组件之一，支撑了flink的每一个作业的运行。它将运行子任务的任务管理器连接在了一起。所以flink的网络栈对我们的吞吐量和低延迟有着至关重要的作用。flink里面的网络栈使用了两个著名的组件：Akka和Netty。

- 作业管理器和任务管理器之间的通信使用Akka组件。
- 任务管理器之间的通信使用更加底层的API：Netty库。

接下来，我们首先从更高层次的抽象来看一下操作符之间的通信，然后再详细研究一下flink的物理实现以及flink所做的各种优化。我们回简要的讲解一下flink做的优化以及flink在吞吐量和延迟之间所做的权衡。

### 逻辑视图

flink的网络栈提供了以下子任务之间通信的逻辑视图，例如`keyBy()`所需要的网络shuffle。

![](./image/flink-network-stack1.png)

这里我们抽象出三种概念：

* 子任务输出类型（ResultPartitionType）：
  * 流水线式发送数据 (有界或者无界数据流): 一旦数据产生，立即向下游发送数据。无论有界数据集还是无界数据集，都采用one-to-one的方式向下游发送。
  * 阻塞式发送数据: 只有当完整的计算结果产生时才向下游发送计算结果。
* 调度类型:
  * 一次全都部署的方式 (激进的调度方式): 将一个作业的所有子任务同时部署（针对流式应用）。
  * 当上游发送数据时部署当前子任务 (惰性的调度方式): 当算子的生产者（也就是上游算子）生产数据的时候才部署这个算子子任务。
  * 上游产生完整计算结果时部署当前子任务: 当算子的生产者完成它的所有计算，并发送完整的计算结果时，才部署当前子任务。
* 数据传输:
  * 高吞吐: flink将需要发送的数据缓存到网络缓存中，然后一起发送它们。而不是一条数据一条数据的发送。这将会降低均摊到每条数据的发送代价，也就提高了吞吐量。
  * 低延迟: 我们可以通过发送一个没有完全填满的网络缓冲区的数据，来降低发送数据的延时。当然，这样我们就为了低延迟而牺牲了吞吐量。

接下来，我们将看一下吞吐量和低延迟方面的网络栈优化。我们会详细了解以下输出和调度类型。首先，我们需要明白，子任务的输出类型和调度类型是紧密交织在一起的。

流水线式的数据传输的`ResultPartition`是一种流式风格的输出。所以我们需要一个运行起来的目标子任务来接收发送的数据。所以目标子任务必须在上游的计算结果或者第一条输出产生之前就调度部署跑起来。批处理作业处理的是有界数据`ResultPartition`而流处理作业处理的是无界数据`ResultPartition`。

批处理作业在生产计算结果时可能是以阻塞的方式生产的，这取决于操作符和使用的连接模式。在这种情况下，必须计算出完整的计算结果，然后接收计算结果的任务才会被部署。这允许批处理作业更加有效的工作，且消耗更少的计算资源。

下面的图标总结了正确的组合：

| 输出类型 | 调度类型 | 应用场景 |
|---------|---------|---------|
| 流水线式输出，无界数据流 | 立即全部部署 | 流处理作业 |
| 流水线式输出，有界数据集 | 接收到第一条数据才部署下一阶段 | 批处理作业 |
| 阻塞式数据输出 | 上游产生完整计算结果再部署下一阶段 | 批处理作业 |

另外，对于超过一个输入的子任务，调度以两种方式开始：等待所有输入都输入数据再部署或者等待某一个输入开始输入数据再部署。

## 物理传输

为了理解数据在物理层的传输，我们先回忆一下，在Flink中，不同的任务是通过`任务槽共享组`来共享同一个任务槽。任务管理器可能会提供超过一个任务槽来允许同一个任务的不同子任务调度部署到同一个任务管理器。

例如下面这幅图，我们假设任务的并行度式4，而两个任务管理器分别提供两个任务槽。任务管理器1执行子任务A.1，A.2，B.1和B.2。而任务管理器2执行的子任务是A.3，A.4，B.3和B.4。如果任务A和任务B的连接是shuffle类型的，例如使用了keyBy()，那么在每个任务管理器上就会有2x4个逻辑连接需要处理，一些连接是本地的，一些是远程的。

<center>
<table class="tg">
  <tr>
    <th></th>
    <th class="tg-wide">B.1</th>
    <th class="tg-wide">B.2</th>
    <th class="tg-wide">B.3</th>
    <th class="tg-wide">B.4</th>
  </tr>
  <tr>
    <th class="tg-wide">A.1</th>
    <td class="tg-center" colspan="2" rowspan="2">本地</td>
    <td class="tg-center" colspan="2" rowspan="2">远程</td>
  </tr>
  <tr>
    <th class="tg-wide">A.2</th>
  </tr>
  <tr>
    <th class="tg-wide">A.3</th>
    <td class="tg-center" colspan="2" rowspan="2">远程</td>
    <td class="tg-center" colspan="2" rowspan="2">本地</td>
  </tr>
  <tr>
    <th class="tg-wide">A.4</th>
  </tr>
</table>
</center>

在Flink网络栈中，不同的任务之间的每一个网络连接都需要有自己的TCP通道。尽管如此，如果同一个任务的不同子任务被调度到了同一个任务管理器，而这些子任务又要和另一个任务管理器进行通信，那么这些子任务将会复用同一个TCP通道。在我们的例子中，将会导致：A.1 → B.3, A.1 → B.4, 以及 A.2 → B.3, 和 A.2 → B.4。

![](./image/flink-network-stack2.png)

每一个子任务产生的结果称为`ResultPartition`，而每一个`ResultPartition`都会切分成几个`ResultSubpartitions`。每一个`ResultSubpartion`都占用了一个逻辑通道。而这时，Flink将不会再处理单条的数据了。而是会去处理一组经过序列化的数据。也就是将这些数据打包放进网络缓冲区中。子任务的缓冲区由它们自己的本地缓冲池来提供：一个发送缓冲区和一个接收缓冲区。缓冲区的大小由下面的公式决定：

```
#channels * buffers-per-channel + floating-buffers-per-gate
```

而一个任务管理器的缓冲数量通常不需要配置。

### 背压问题（1）

当一个子任务的发送数据缓冲池已经被填满。也就是说不管是`ResultSubpartition`的缓冲队列还是更加底层的Netty这一层的网络栈的缓冲区都已经被填满了。那么这个时候生产者就被阻塞了，也就是产生了我们所说的背压问题。而接受者也已相同的原理工作，如果想要继续接收数据，那么Netty需要提供足够的缓冲区来接收数据。如果子任务的缓冲池无法提供足够的网络缓冲区，那么接受者子任务将会停止接收数据。直到缓冲区空出来，才会继续接收数据。这当然也会产生背压问题，因为所有的输入数据的子任务都无法发送数据了，或者说共享的多路复用通道被占满了，导致其他的子任务也无法接收数据了。下面的图中展示了当子任务B.4负载过高以后，将会引起多路复用通道的背压，所以子任务B.3也无法接收数据了。即使子任务B.3还有接收数据的空间。

![](./image/flink-network-stack3.png)

为了防止这种情况出现，Flink引入了自己的流控制机制。

## 基于信用的流控制

基于信用的流控制保证了接受者子任务一直会有足够的空间来接收数据。这依赖于网络缓冲区是Flink的扩展机制这一特性。每一个远程输出通道都有自己的单独的缓冲区，而不是只有一个共享的本地缓冲池。相反，本地的缓冲池被称为浮动缓冲区，因为这个缓冲区会“浮动”，也就是会提供给每一个输入通道。

接受者将会告诉发送者当前可用的缓冲区（也就是信用，1 buffer = 1 credit）。每一个`ResultSubpartition`将会记录它的通道的信用值。如果有足够的信用值，缓冲区中的数据（和信用值一样大小）将会被发送到更加底层的网络栈，每一个发送的数据都会使信用值减一。除了需要发送数据以外，还需要发送当前backlog的大小，也就是当前还有多少数据需要发送（有多少数据在`subpartition`的队列中）。接受者将会使用这个信息来请求一个合适数量的缓冲（从浮动缓冲区申请）以加快backlog数据的处理。接收者将会尽量从浮动缓冲区中请求backlog大小的一些缓冲，当然，浮动缓冲区中的缓冲数量不一定够。所以接收者可能只能请求到一些缓冲，也可能完全请求不到任何缓冲。接收者将会使用申请到的这些缓冲，然后继续等待更多的浮动缓冲来使用。

![](image/flink-network-stack4.png)

基于信用的流控制使用了`每通道缓冲数量`来指定多大的缓冲区是通道独有的独立缓冲区。然后使用`每个门的浮动缓冲数量`来指定本地缓冲池的大小。这样就获得了如同没有流控制时候的缓冲区的大小。这两个参数的默认值的设置，使得流控制的最大吞吐量和不使用流控制的最大吞吐量是一样的。还实现了更加健壮的网络IO功能，从而保证了低延迟。我们可以根据实际情况来调节这两个参数。

### 背压问题（2）

和没有使用流控制机制的接收者的背压机制相比，基于信用的流控制提供了一种更加直接的控制：如果一个接收者没有跟上发送者的节奏，也就是接收者信用值变成了0，那么将会让发送者停止从缓冲区发送数据到网络栈。所以只会在当前接收者的逻辑通道上产生背压问题，所以没有必要去阻塞从共享的多路复用的TCP通道读取数据。所以其他接收者还可以继续接收数据，不会受到影响。

### 我们收获了什么？关键点在哪里？

由于我们使用了流控制，一个通道在使用多路复用的TCP通道时并不会阻塞到其他的逻辑通道。所以资源的使用率会上升。另外，由于我们可以彻底控制数据的传输，所以我们可以改进检查点分界线对齐的性能。如果没有流控制，那么逻辑通道会把网络栈的内部缓冲区慢慢的填满，进而使得接收者不再接收数据。在无法接收数据的这段时间，一大堆数据只好在原地呆着了。而检查点分界线排在这些数据的后面，等着向下游发送。只有当这些数据都发送完了，检查点分界线才会跟着向下游发送。因为检查点分界线只是一个特殊的事件，并不会弯道超车，越过其他数据向下游发送。

当然，由于我们需要在接收者发送一些额外的数据（信用值），所以需要花费一些额外的代价，尤其是在建立SSL加密的连接通道时。一个单独的输入通道无法使用缓冲池中的所有缓冲，因为缓冲是针对每个输入通道独立的，不是共享的。如果我们生产数据的速度过快，这些数据也可能无法尽可能多的发送到下游去，因为我们必须等待下游的算子将信用值发送过来，才能发送数据，这样无形中也会使发送数据的时间更长。这可能会影响我们的作业执行的性能。即使这样，我们也需要使用流控制，因为它的好处太大了。我们可以通过增加每个通道所独有的缓冲区的大小（通过调节buffers-per-channel参数）来缓解这一问题，当然这会更加的耗费内存。即使这样，比之没有流控制的实现来说，我们的内存总占用量也会更加少。因为更加底层的网络栈缓冲区（Netty这一层）并不需要缓存很多数据，因为稍微缓存一点数据就会发送到下游去。

还有一点需要注意，就是当使用基于信用的流控制时：由于我们在发送者和接收者之间缓存了较少的数据，所以我们可能会更早的碰到背压问题。这是必须的，因为缓存更多的数据没有任何好处。如果我们想要缓存更多的数据，同时保持流控制机制的话，可以考虑调节浮动缓冲区的大小（通过调节floating-buffers-per-gate参数）。