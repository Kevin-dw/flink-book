\documentclass[cn,11pt,chinese]{elegantbook}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\usepackage{epigraph}



\title{Flink实践指南}
\subtitle{Flink理论与实战}

\author{左元}
\institute{尚硅谷}
\date{April 12, 2020}
\version{0.1}

\extrainfo{完成比完美更重要！}
\logo{logo-blue.png}
\cover{cover.jpg}

\usepackage{array}
\newcommand{\ccr}[1]{\makecell{{\color{#1}\rule{1cm}{1cm}}}}

\tikzset{
box/.style ={
rectangle, %矩形节点
rounded corners =5pt, %圆角
minimum width =50pt, %最小宽度
minimum height =20pt, %最小高度
inner sep=5pt, %文字和边框的距离
draw=blue %边框颜色
}
}

\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=red!30]
\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!30]
\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=4cm, draw=black, fill=orange!30]
\tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=green!30]
\tikzstyle{arrow} = [thick,->,>=stealth]
\begin{document}

\begin{tikzpicture}
  \draw (0,0) -- (4,0);
\end{tikzpicture}

\maketitle
\frontmatter

\tableofcontents

\mainmatter
\part{Flink的使用}

\chapter{流式处理理论概述}

\chapter{Flink框架快速上手}

% 以下三章先使用处理时间讲解，不引进事件时间。

\chapter{Flink DataStream API}

\chapter{基于时间和窗口的算子}

\chapter{Flink状态编程}

% 引进事件时间和水位线

\chapter{通过编程理解事件时间和水位线}

\chapter{Flink批处理API}

\chapter{Flink与外部系统的交互}

\chapter{Flink Table API \& SQL}

\chapter{Flink CEP库的使用}

\part{Flink的部署与运维}

\chapter{Flink应用的监控}

\chapter{如何部署Flink集群}

\chapter{Flink集群的高可用}

\part{Flink内核与优化}

\chapter{Flink运行时架构}

\chapter{Flink状态的原理}

\chapter{Flink的容错机制}

\chapter{Flink作业的调度}

\chapter{Flink内存管理的特点}

\chapter{Flink的网络IO机制}

\chapter{Flink常见优化措施}

\epigraph{过早优化是万恶之源。}{高德纳}

任何程序的编写其实遵循三个阶段。第一，编写出一个可以运行的程序。第二，确保这个程
序是正确的。第三，优化这个程序。所以高德纳才说：“过早优化是万恶之源”。而Flink
程序与其他的程序的优化策略的基本道理是一样的，也就是尽量减少计算密集型和IO密集型
这两种运算。

我们先来看一下在开发Flink程序时需要注意的一些要点和常犯的一些错误：

首先Flink的程序开发流程如下图所示：

\begin{tikzpicture}
\node[box] (1) at(0,0) {项目启动};
\node[box] (2) at(4,0) {需求分析};
\node[box] (3) at(8,0) {程序开发};
\node[box] (4) at(0,-4) {(测试)};
\node[box] (5) at(4,-4) {部署};
\node[box] (6) at(8,-4) {维护};
\draw[->] (1)--(2);
\draw[->] (2)--(3);
\draw[->] (3)--(4);
\draw[->] (4)--(5);
\draw[->] (5)--(6);
\end{tikzpicture}

\section{项目启动}

在项目启动时，如果我们想要使用Flink来进行开发，那么可能会犯以下一些错误：

\begin{itemize}
\item 不使用一种迭代式的开发，而是想要毕其功于一役。
\item 刚学了一些Flink基础知识，就想用Flink来解决公司最难的需求。
\item 事先没有流处理的知识，直接开始使用Flink进行编程。
\item 事先没有经过Flink编程方面的训练，例如：基本API的熟悉，基本的调优训练等等。
\item 不和社区进行交流，不在社区中搜索可能已经存在的答案。例如不使用
  StackOverflow、Flink Documentation、Flink Jira等常见工具。
\end{itemize}

\section{分析需求}

\begin{itemize}
\item 不考虑程序的一致性，例如是否允许丢失数据，是否允许数据重复计算。

下面的流程图展示了我们在写Flink程序时，有关数据一致性方面，需要考虑到的一些选项。

在使用Flink编写端到端(source -> transformation -> sink)的流处理程序时有这样一些一致性级别：

\begin{itemize}
\item at-most-once: 如果应用场景为传感器数据的实时监控，则丢失数据也是可以接受的。
\item at-least-once
\item exactly-once
\end{itemize}

\begin{tikzpicture}[node distance=2cm]
 %定义流程图具体形状
\node (1) [process] {是否关心数据丢失？};
\node (2) [process, below of=1] {需要结果的正确吗？};
\node (3) [process, right of=2, xshift=5cm] {任何数据源，无须做检查点};
\node (4) [process, below of=2] {不允许重复发送正确的计算结果吗？};
\node (5) [process, right of=4, xshift=5cm] {CheckpointingMode\\.AT\_LEAST\_ONCE且数据源可重置};
\node (6) [process, below of=4, yshift=-1cm] {CheckpointingMode\\.EXACTLY\_ONCE且数据源可重置且事务性写入第三方设备};
\node (7) [process, right of=6, xshift=5cm, yshift=-1cm] {CheckpointingMode\\.EXACTLY\_ONCE且数据源可重置};

 %连接具体形状
\draw [arrow](1) -- node [anchor=east] {是} (2);
\draw [arrow](1) -- node [anchor=south] {否} (3);
\draw [arrow](2) -- node [anchor=east] {是} (4);
\draw [arrow](2) -- node [anchor=south] {否} (5);
\draw [arrow](4) -- node [anchor=east] {是} (6);
\draw [arrow](4) -- node [anchor=south] {否} (7);
\end{tikzpicture}
  
\item 不考虑程序后续的迭代和升级
\item 不考虑需要解决的问题的规模，例如数据量的大小，数据流高峰期的数据量等等。
\item 没有细致的分析真实的业务需求
\end{itemize}

\section{开发}

\begin{itemize}
\item 没有细致思考到底使应该使用DataStream API还是Table API \& SQL？

  我们先来看一下DataStream API的特点：

  \begin{itemize}
  \item 流中的数据类型是Java或者Scala的Class类型。
  \item 使用的是一系列转换操作符。
  \item 程序按照我们编写的操作符的顺序运行。
  \item 可以对状态进行显式的控制。
  \item 可以对时间语义进行显式的控制。
  \end{itemize}

  而Table API \& SQL本质上是声明式编程，所以很多底层的细节我们从代码中无法很容易
  的看出来。SQL的特点是：

  \begin{itemize}
  \item 我们可以声明表的Schema，也就是表结构。
  \item 我们可以使用SQL这样通用的声明式编程语言。
  \item Flink底层会为我们进行优化，也就意味着我们无法从SQL层面来进行一些细致的
      优化。
  \item 从我们编写的SQL程序中看不出状态是如何被操作的。
  \item Flink底层决定何时去触发（Trigger）计算的执行。
  \end{itemize}

  那么我们什么时候避免使用Flink SQL呢？

  \begin{itemize}
  \item 当升级Flink版本或者我们自己编写的程序需要升级时，我们需要迁移状态。
  \item 程序运行时，迟到数据不能被丢弃。
  \item 在运行时，希望动态的改变程序的行为。
  \end{itemize}
  
\item 对Flink的类型系统理解不深，盲目使用了嵌套过深或者非常复杂的数据结构（例如
  深度嵌套的POJO CLASS或者case class）

  我们在编写Flink程序的时候，流里面的数据类型如果是POJO CLASS或者CASE CLASS，那
  么字段类型最好是基本数据结构，例如Integer，String等。而不是其他嵌套的POJO或者
  样例类。如果数据类型本身很复杂，例如嵌套很深的JSON字符串，那么最好将所有没用的
  字段全部使用map操作符清晰掉，并展平成结构简单的数据结构。例如：

  \begin{lstlisting}
    {
      "name": "atguigu",
      "age": 10
    }

  \end{lstlisting}
  
\item 胡乱使用keyBy()函数

  由于KeySelectors中的getKey方法可以返回任意的序列化类型，所以随便定义Key。而Key
  最好是简单数据类型，如String，Tuple，POJO等。

  我们可以看一下各种序列化类型的代价：

  \begin{table}[htbp!]
    \centering
    \caption{序列化类型的代价}
    \begin{tabular}{ll}
      \toprule
      序列化器 & Objects Per Second \\
      \midrule
      PojoSerializer & 813 \\
      Kryo & 294 \\
      Avro (Reflect API) & 114 \\
      Avro (SpecificRecord API) & 632 \\
      \bottomrule
    \end{tabular}%
  \end{table}%

  通过上面的表可以看出，Key的类型至关重要，因为Key会作为状态被保存下来，定时器作
  为状态被保存也同时需要保存Key。而序列化的时间复杂度是不能忽略的。所以我们尽量
  使用简单的数据类型，例如：Integer、String、POJO类和Avro SpecificRecord类型等。
  总而言之，Key的类型越简单越好。

  \begin{lstlisting}
  sourceStream
      // 在反序列化器中，要将数据反序列化的越简单越好！
      .flatMap(new Deserializer())
      // key最好要很简单！
      .keyBy("key")
      .timeWindow()
      .count()
      .addSink()
  \end{lstlisting}

  例如不要将没用的字段保留下来，如下面的例子：

  \begin{lstlisting}
    public class Event {
      public String key;
      // 没用的字段都去掉！
      public String unusedField;
    } 
  \end{lstlisting}
  
\item 在不同的任务之间共享静态变量，从而造成了死锁、竞争等同步问题。
\item 在UDF函数中随意开启新的线程，会造成非常难以调试的bug，例如检查点相关的问题。
\item 随便自定义窗口函数，而没有使用Flink默认的函数。
\item 没有将初始化的代码放在富函数的open方法中，而是放在了例如flatMap、map、
  filter函数中。
\end{itemize}

\section{测试}

\section{部署}

\section{维护}

由于Flink更新非常频繁，所以不要随便对程序进行升级。

\chapter{流的去重及其优化}

\chapter{如何解决数据倾斜}

\part{Flink实时数仓项目}

\end{document}
